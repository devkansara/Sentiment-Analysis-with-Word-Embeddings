{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements:\n",
    "Python 3.9, I have used torch.device(\"mps\") which is cuda equivalent for MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/v8f_4jrn3xl3ljkf0f1bzmw80000gn/T/ipykernel_3327/2700134964.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/v8f_4jrn3xl3ljkf0f1bzmw80000gn/T/ipykernel_3327/1014463926.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the dataset into 5 different datasets based on the star_rating\n",
    "rating1=dataset.loc[dataset['star_rating'].isin([1])].sample(n=50000, random_state=36)\n",
    "rating2=dataset.loc[dataset['star_rating'].isin([2])].sample(n=50000, random_state=36)\n",
    "rating3=dataset.loc[dataset['star_rating'].isin([3])].sample(n=50000, random_state=36)\n",
    "rating4=dataset.loc[dataset['star_rating'].isin([4])].sample(n=50000, random_state=36)\n",
    "rating5=dataset.loc[dataset['star_rating'].isin([5])].sample(n=50000, random_state=36)\n",
    "finaldf = pd.concat([rating1, rating2, rating3, rating4, rating5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n",
    "df = finaldf[['star_rating', 'review_headline', 'review_body']]\n",
    "df.rename(columns={'star_rating': 'ratings'}, inplace=True)\n",
    "df['review_headline'] = df['review_headline'].apply(str)\n",
    "df['review_body'] = df['review_body'].apply(str)\n",
    "df['reviews'] = df[['review_headline', 'review_body']].agg(' '.join, axis=1)\n",
    "df = df.drop('review_headline', axis=1)\n",
    "df = df.drop('review_body', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating Classes\n",
    "df['class'] = df['ratings'].apply(lambda x: 1 if x > 3 else 2 if x < 3 else 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/v8f_4jrn3xl3ljkf0f1bzmw80000gn/T/ipykernel_3327/1568710013.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  df['reviews'] = df['reviews'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
      "[nltk_data] Downloading package stopwords to /Users/dev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Convert to lower case\n",
    "df['reviews'] = df['reviews'].str.lower()\n",
    "# Remove HTML tags\n",
    "df['reviews'] = df['reviews'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "# Remove URLs\n",
    "df['reviews'] = df['reviews'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# Remove non-alphabetical characters\n",
    "df['reviews'] = df['reviews'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "# Remove extra spaces\n",
    "df['reviews'] = df['reviews'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Performing contractions\n",
    "contractions = {\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"i will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "}\n",
    "\n",
    "df['reviews'] = df['reviews'].replace(contractions, regex=True)\n",
    "\n",
    "#Removing Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['reviews'] = df['reviews'].apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['reviews'] = df['reviews'].apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: \n",
      "Analogy Result: [('queen', 0.7118192911148071)]\n",
      "Similarity Score between 'excellent' and 'outstanding': 55.6748628616333%\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained: \")\n",
    "# Load the pretrained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Example 1: Finding the analogy \"King - Man + Woman = Queen\"\n",
    "analogy_result = word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"Analogy Result:\", analogy_result)\n",
    "\n",
    "# Example 2: Finding semantic similarity between \"excellent\" and \"outstanding\"\n",
    "similarity_score = word2vec_model.similarity('excellent', 'outstanding') * 100\n",
    "\n",
    "print(f\"Similarity Score between 'excellent' and 'outstanding': {similarity_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [str(sentence).split() for sentence in df['reviews'].values]\n",
    "model_own = Word2Vec(sentences, vector_size=300, window=11, min_count=10)\n",
    "#Saving Model\n",
    "model_own.save(\"word2vec_model_own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model\n",
    "model_own = Word2Vec.load(\"word2vec_model_own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own Model: \n",
      "Analogy: king - man + woman = population\n",
      "Similarity between 'excellent' and 'outstanding': 71.83629274368286%\n"
     ]
    }
   ],
   "source": [
    "print(\"Own Model: \")\n",
    "analogy_result = model_own.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(f\"Analogy: king - man + woman = {analogy_result[0][0]}\")\n",
    "\n",
    "similarity_score = model_own.wv.similarity('excellent', 'outstanding') * 100\n",
    "print(f\"Similarity between 'excellent' and 'outstanding': {similarity_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the Analogy is more accurate on the pretrained model, whereas the similarity score between excellent and outstanding is more in our custom trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy(deep=True)  \n",
    "\n",
    "df1 = df.loc[:, [\"class\", \"reviews\"]]\n",
    "class1 = df1[df1['class'] == 1]  \n",
    "class2 = df1[df1['class'] == 2]  \n",
    "df1 = pd.concat([class1, class2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron & SVM for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets with an 80%/20% split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(df1['reviews'], df1['class'], test_size=0.2, random_state=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "#Transform the test data using the same vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#Now, X_train_tfidf and X_test_tfidf contain the TF-IDF features for training and testing sets\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "X_test_df = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: Perceptron Accuracy: 83.87%\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = Perceptron(random_state=42)\n",
    "perceptron_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on training set\n",
    "train_predictions = perceptron_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on testing set\n",
    "test_predictions = perceptron_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics on the testing set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions) * 100\n",
    "\n",
    "# Print the metrics\n",
    "print(f'TF-IDF: Perceptron Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/homebrew/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - SVM Accuracy: 88.1375%\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM model\n",
    "svm_model = LinearSVC(max_iter=5000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on training set\n",
    "train_predictions = svm_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on testing set\n",
    "test_predictions = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics on the training set\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "# Calculate metrics on the testing set\n",
    "test_accuracy = accuracy_score(y_test, test_predictions) * 100\n",
    "\n",
    "# Print the metrics\n",
    "print(f'TF-IDF - SVM Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron & SVM for custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word2vec(review, wv1):\n",
    "    if isinstance(review, str):  \n",
    "        important_words = [word for word in review.split() if word in wv1]\n",
    "        if important_words:\n",
    "            return np.mean([wv1[word] for word in important_words], axis=0)\n",
    "        else:\n",
    "            return np.zeros(wv1.vector_size)\n",
    "    else:\n",
    "        return np.zeros(wv1.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([average_word2vec(reviews, model_own.wv) for reviews in df1['reviews']])\n",
    "y = df1['class'].values \n",
    "y = np.array(y) - 1 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom - Perceptron Accuracy: 82.3675%\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "perceptron_accuracy = perceptron_model.score(X_test, y_test) * 100\n",
    "\n",
    "print(f'Custom - Perceptron Accuracy: {perceptron_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/homebrew/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom - SVM Accuracy: 88.1375%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train a linear SVM model\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_accuracy = svm_model.score(X_test, y_test) * 100\n",
    "\n",
    "print(f'Custom - SVM Accuracy: {svm_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple models for pretrained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained - Perceptron Accuracy: 71.5225%\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([average_word2vec(reviews, word2vec_model) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1 \n",
    "\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=31)\n",
    "\n",
    "# Train a perceptron model\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X1_train, y_train)\n",
    "\n",
    "# Evaluate the perceptron model\n",
    "perceptron_accuracy = perceptron_model.score(X1_test, y_test) * 100\n",
    "\n",
    "print(f'Pretrained - Perceptron Accuracy: {perceptron_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dev/homebrew/lib/python3.9/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy (word2vec-google-news-300): 84.34%\n"
     ]
    }
   ],
   "source": [
    "# Train a linear SVM model\n",
    "svm_model = LinearSVC(max_iter=5000)\n",
    "svm_model.fit(X1_train, y_train)\n",
    "\n",
    "svm_accuracy = svm_model.score(X1_test, y_test)*100\n",
    "\n",
    "print(f'Linear SVM Accuracy (word2vec-google-news-300): {svm_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that TF-IDF SVM accuracy is the highest amongst all other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom word2vec Binary - Testing Accuracy: 89.585%\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([average_word2vec(reviews, model_own.wv) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X1.shape[1]\n",
    "model = MLP(input_size, num_classes=2)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Custom word2vec Binary - Testing Accuracy: {accuracy}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word2vec Binary - Testing Accuracy: 87.3975%\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([average_word2vec(reviews, word2vec_model) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X1.shape[1]\n",
    "model = MLP(input_size, num_classes=2)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Pretrained word2vec Binary - Testing Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom word2vec Ternary - Testing Accuracy: 76.13 %\n"
     ]
    }
   ],
   "source": [
    "#Train feedforward neural network for all three classes\n",
    "X2 = np.array([average_word2vec(reviews, model_own.wv) for reviews in df['reviews']])\n",
    "y2 = df['class'].values\n",
    "y2 = np.array(y2) - 1\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=31)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X2_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X2_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "input_size = X2.shape[1]\n",
    "model = MLP(input_size, num_classes=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "#    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Custom word2vec Ternary - Testing Accuracy: {accuracy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word2vec Ternary - Testing Accuracy: 73.402 %\n"
     ]
    }
   ],
   "source": [
    "#Train feedforward neural network for all three classes\n",
    "X2 = np.array([average_word2vec(reviews, word2vec_model) for reviews in df['reviews']])\n",
    "y2 = df['class'].values\n",
    "y2 = np.array(y2) - 1\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=31)\n",
    "\n",
    "X_train_tensor = torch.tensor(X2_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X2_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "input_size = X2.shape[1] \n",
    "model = MLP(input_size, num_classes=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "#    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Pretrained word2vec Ternary - Testing Accuracy: {accuracy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word2vec_concat10(review, wv):\n",
    "    words = review.split()[:10]\n",
    "    vectors = [wv[word] for word in words if word in wv]\n",
    "\n",
    "    if len(vectors) < 10:\n",
    "        vectors += [np.zeros(wv.vector_size)] * (10 - len(vectors))\n",
    "    return np.concatenate(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Concat Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Concat word2vec Binary - Testing Accuracy: 84.2 %\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([average_word2vec_concat10(reviews, model_own.wv) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X1.shape[1]\n",
    "model = MLP(input_size, num_classes=2)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Custom Concat word2vec Binary - Testing Accuracy: {accuracy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Concat binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Concat word2vec Binary - Testing Accuracy: 83.05 %\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([average_word2vec_concat10(reviews, word2vec_model) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1 \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X1.shape[1]\n",
    "model = MLP(input_size, num_classes=2)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Pretrained Concat word2vec Binary - Testing Accuracy: {accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Concat Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Concat word2vec Ternary - Testing Accuracy: 70.766 %\n"
     ]
    }
   ],
   "source": [
    "X2 = np.array([average_word2vec_concat10(reviews, model_own.wv) for reviews in df['reviews']])\n",
    "y2 = df['class'].values\n",
    "y2 = np.array(y2) - 1\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X2_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X2_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "input_size = X2.shape[1] \n",
    "model = MLP(input_size, num_classes=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "   # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Custom Concat word2vec Ternary - Testing Accuracy: {accuracy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained concat Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ternary model on the test set: 69.174 %\n"
     ]
    }
   ],
   "source": [
    "X2 = np.array([average_word2vec_concat10(reviews, word2vec_model) for reviews in df['reviews']])\n",
    "y2 = df['class'].values\n",
    "y2 = np.array(y2) - 1\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X2_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y2_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X2_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y2_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "input_size = X2.shape[1] \n",
    "model = MLP(input_size, num_classes=3)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the ternary model on the test set: {accuracy} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that Custom word2vec Binary's Testing Accuracy is more than any other simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(30000, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1) if num_classes > 2 else nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec50(review, wv):\n",
    "    length = 50\n",
    "    words = str(review)[:length].split()[:10]\n",
    "    vectors = [wv[word] for word in words if word in wv]\n",
    "    if len(vectors) < 10:\n",
    "        vectors += [np.zeros(wv.vector_size)] * (10 - len(vectors))\n",
    "    return np.concatenate(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, criterion, optimizer, train_loader, test_loader, num_epochs):\n",
    "    device = torch.device(\"mps\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        #print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #print(f'{(100 * correct / total)} %')\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Binary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([word2vec50(reviews, word2vec_model) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=df1['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = CNN(2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.554\n",
      "Epoch 2 loss: 0.488\n",
      "Epoch 3 loss: 0.477\n",
      "Epoch 4 loss: 0.471\n",
      "Epoch 5 loss: 0.467\n",
      "Pretrained Binary CNN - Test Accuracy: 83.2125\n"
     ]
    }
   ],
   "source": [
    "print(f'Pretrained Binary CNN - Test Accuracy: {train_and_test_model(binary_model, criterion, binary_optimizer, train_loader, test_loader, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Binary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([word2vec50(reviews, model_own.wv) for reviews in df1['reviews']])\n",
    "y = df1['class'].values\n",
    "y = np.array(y) - 1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=df1['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = CNN(2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Binary CNN - Test Accuracy: 85.5675\n"
     ]
    }
   ],
   "source": [
    "print(f'Custom Binary CNN - Test Accuracy: {train_and_test_model(binary_model, criterion, binary_optimizer, train_loader, test_loader, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained ternary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([word2vec50(reviews, word2vec_model) for reviews in df['reviews']])\n",
    "y = df['class'].values\n",
    "y = np.array(y) - 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=df['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_model = CNN(3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Ternary CNN - Test Accuracy: 67.28\n"
     ]
    }
   ],
   "source": [
    "print(f'Pretrained Ternary CNN - Test Accuracy: {train_and_test_model(binary_model, criterion, binary_optimizer, train_loader, test_loader, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Ternary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([word2vec50(reviews, model_own.wv) for reviews in df['reviews']])\n",
    "y = df['class'].values\n",
    "y = np.array(y) - 1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=df['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_model = CNN(3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Ternary CNN - Test Accuracy: 68.248\n"
     ]
    }
   ],
   "source": [
    "print(f'Custom Ternary CNN - Test Accuracy: {train_and_test_model(binary_model, criterion, binary_optimizer, train_loader, test_loader, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that Custom modal's Binary CNN architecture had the highest accuracy of 85.5675% amongst the other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
